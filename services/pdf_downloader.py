#!/usr/bin/env python3
"""
ì§€ìì²´ ì˜íšŒ ì—…ë¬´ì¶”ì§„ë¹„ ê³µê°œ í˜ì´ì§€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ v5.0
í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ 3ê°œì›”ì¹˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (í˜„ì¬ì›” í¬í•¨)

v5.0 ì£¼ìš” ê°œì„ ì‚¬í•­:
- ë³‘ë ¬ ì²˜ë¦¬ ë„ì… (ë©€í‹°í”„ë¡œì„¸ì‹±): ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ë™ì‹œ ì²˜ë¦¬ë¡œ ì‹¤í–‰ ì‹œê°„ ë‹¨ì¶•
- íŒŒì¼ëª… ì¸ì½”ë”©/ë””ì½”ë”© ì²˜ë¦¬ ê°•í™”: UTF-8, EUC-KR, CP949 ë“± ë‹¤ì–‘í•œ ì¸ì½”ë”© ì§€ì› ê°œì„ 
- ì‚¬ì´íŠ¸ëª… ì¶”ì¶œ ë¡œì§ ê°•í™”: "www" ëŒ€ì‹  ì •í™•í•œ ì§€ìì²´ëª… ë°˜ì˜
- ë‹¤ìš´ë¡œë“œ ì¬ì‹œë„ ë¡œì§ ê°•í™”: "ë°”ë¡œë³´ê¸°" ë§í¬ ì²˜ë¦¬ ê°œì„ 
- ë¡œê¹… ë° ì§„í–‰ í‘œì‹œ ê°œì„ : tqdm ì§„í–‰ë°” ì¶”ê°€
"""

import os
import re
import time
import json
import logging
import hashlib
import random
import concurrent.futures
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse, unquote, parse_qs, urlencode, quote
import requests
from bs4 import BeautifulSoup
from typing import Dict, List, Tuple, Optional, Set, Any, Union
import warnings
warnings.filterwarnings('ignore')
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor


EXECUTOR_KIND_DEFAULT = os.getenv("DOWNLOADER_EXECUTOR", "process").lower()
RUNNING_IN_FLASK_DEFAULT = os.getenv("RUN_FROM_FLASK", "0").lower() in ("1", "true", "yes", "y")

def process_sites_parallel(self, urls: List[str]) -> List[Dict]:
    all_stats = []

    # ìˆœì°¨
    if self.max_workers <= 1:
        for i, url in enumerate(urls, 1):
            logger.info(f"\n{'â–¶'*3} ì§„í–‰: {i}/{len(urls)} ({i/len(urls)*100:.1f}%)")
            stats = self.process_site(url)
            all_stats.append(stats)
            self.stats[stats['site_name']] = stats
            if i < len(urls):
                time.sleep(random.uniform(1.0, 2.0))
        return all_stats

    # â˜… Flask ë‚´ë¶€ì—ì„œëŠ” í”„ë¡œì„¸ìŠ¤ í’€ ê¸ˆì§€ â†’ ìŠ¤ë ˆë“œ í’€ ì‚¬ìš©
    if self.running_in_flask or self.executor_kind == "thread":
        logger.info(f"ğŸ§µ ThreadPoolExecutor ì‚¬ìš© (workers={self.max_workers})")
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            if TQDM_AVAILABLE:
                for stats in tqdm(executor.map(self.process_site, urls),
                                  total=len(urls), desc="ì‚¬ì´íŠ¸ ì²˜ë¦¬ ì¤‘", unit="site"):
                    all_stats.append(stats)
            else:
                for stats in executor.map(self.process_site, urls):
                    all_stats.append(stats)
        for st in all_stats:
            self.stats[st['site_name']] = st
        return all_stats

    # (ì˜µì…˜) ë…ë¦½ í”„ë¡œì„¸ìŠ¤ ëª¨ë“œ â€” CLIì—ì„œë§Œ (Flask X)
    # ë°”ìš´ë“œ ë©”ì„œë“œ í”¼í´ë§ì„ í”¼í•˜ê¸° ìœ„í•´ ëª¨ë“ˆ ìµœìƒìœ„ í•¨ìˆ˜ ì‚¬ìš©
    def _worker_payload(url):
        return {"url": url, "use_selenium": self.use_selenium}

    logger.info(f"âš™ï¸ ProcessPoolExecutor ì‚¬ìš© (workers={self.max_workers})")
    with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
        # tqdm ë¶„ê¸°
        it = executor.map(_worker_process_site, map(_worker_payload, urls))
        results = list(tqdm(it, total=len(urls), desc="ì‚¬ì´íŠ¸ ì²˜ë¦¬ ì¤‘", unit="site")) if TQDM_AVAILABLE else list(it)
        all_stats.extend(results)
    for st in all_stats:
        self.stats[st['site_name']] = st
    return all_stats

# ëª¨ë“ˆ ìµœìƒìœ„(í´ë˜ìŠ¤ ë°–)ì— ì¶”ê°€ â€” í”„ë¡œì„¸ìŠ¤ìš© ì›Œì»¤
def _worker_process_site(payload: dict) -> dict:
    url = payload["url"]
    use_selenium = payload.get("use_selenium", False)
    # ê° í”„ë¡œì„¸ìŠ¤ëŠ” ë…ë¦½ ë‹¤ìš´ë¡œë”(ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ ë©€í‹° ì•ˆì”€)
    dl = CouncilFileDownloader(use_selenium=use_selenium, max_workers=1)
    return dl.process_site(url)

# ì§„í–‰ í‘œì‹œ ë„êµ¬
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False

# Selenium ì˜µì…˜
try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.options import Options
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('download_log.txt', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ë©€í‹°í”„ë¡œì„¸ì‹± ë¡œê·¸ ê²©ë¦¬
from multiprocessing import current_process
class ProcessNameFilter(logging.Filter):
    def filter(self, record):
        record.processName = current_process().name
        return True

for handler in logger.handlers:
    handler.addFilter(ProcessNameFilter())

class FileDeduplicator:
    """íŒŒì¼ ì¤‘ë³µ ì œê±° ê´€ë¦¬ì"""
    
    def __init__(self):
        self.seen_urls = set()
        self.seen_filenames = set()
        self.seen_hashes = set()
        self.url_to_filename = {}
        
    def normalize_url(self, url: str) -> str:
        """URL ì •ê·œí™”"""
        if not url:
            return ""
        
        # ê°œí–‰ë¬¸ì, ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        url = re.sub(r'[\r\n\t]', '', url)
        url = re.sub(r'\s+', ' ', url)
        
        # íŒŒë¼ë¯¸í„° ì •ë ¬
        parsed = urlparse(url)
        if parsed.query:
            params = parse_qs(parsed.query)
            sorted_params = sorted(params.items())
            query = urlencode(sorted_params, doseq=True)
            url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{query}"
        
        # ë¶ˆí•„ìš”í•œ íŒŒë¼ë¯¸í„° ì œê±°
        remove_params = ['timestamp', 'ts', '_', 'random', 'cache']
        parsed = urlparse(url)
        if parsed.query:
            params = parse_qs(parsed.query)
            for param in remove_params:
                params.pop(param, None)
            if params:
                query = urlencode(params, doseq=True)
                url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{query}"
            else:
                url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        
        return url.lower().strip()
    
    def is_duplicate_url(self, url: str) -> bool:
        """URL ì¤‘ë³µ ì²´í¬"""
        normalized = self.normalize_url(url)
        if normalized in self.seen_urls:
            return True
        self.seen_urls.add(normalized)
        return False
    
    def is_duplicate_filename(self, filename: str, url: str = None) -> bool:
        """íŒŒì¼ëª… ì¤‘ë³µ ì²´í¬"""
        key = filename.lower().strip()
        
        # ë™ì¼ URLì—ì„œ ì˜¨ íŒŒì¼ì€ í—ˆìš©
        if url:
            normalized_url = self.normalize_url(url)
            if normalized_url in self.url_to_filename:
                if self.url_to_filename[normalized_url] == key:
                    return False
            self.url_to_filename[normalized_url] = key
        
        if key in self.seen_filenames:
            return True
        self.seen_filenames.add(key)
        return False
    
    def get_file_hash(self, filepath: str) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        hasher = hashlib.md5()
        try:
            with open(filepath, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except Exception:
            return None
    
    def is_duplicate_content(self, filepath: str) -> bool:
        """íŒŒì¼ ë‚´ìš© ì¤‘ë³µ ì²´í¬"""
        file_hash = self.get_file_hash(filepath)
        if not file_hash:
            return False
        
        if file_hash in self.seen_hashes:
            return True
        self.seen_hashes.add(file_hash)
        return False
    
    def get_stats(self) -> Dict:
        """ì¤‘ë³µ ì œê±° í†µê³„"""
        return {
            'unique_urls': len(self.seen_urls),
            'unique_filenames': len(self.seen_filenames),
            'unique_contents': len(self.seen_hashes)
        }

class CouncilFileDownloader:
    def __init__(self, use_selenium=False, max_workers=4):
        
        # User-Agent ëª©ë¡ í™•ì¥ (self.session ìƒì„±ë³´ë‹¤ ë¨¼ì € ì •ì˜ë˜ì–´ì•¼ í•¨)
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Edge/120.0.0.0'
        ]
        
        self.session = self._create_session()
        
        # ì¤‘ë³µ ì œê±° ê´€ë¦¬ì
        self.deduplicator = FileDeduplicator()
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        self.max_workers = max_workers
        
        # Selenium ë“œë¼ì´ë²„
        self.driver = None
        self.use_selenium = use_selenium
        if use_selenium and SELENIUM_AVAILABLE:
            try:
                self._init_selenium()
                logger.info("Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.warning(f"Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        self.current_date = datetime.now()
        self.target_months = self.get_target_months()
        
        self.base_download_dir = f"downloads_{self.current_date.strftime('%Y%m%d_%H%M')}"
        if not os.path.exists(self.base_download_dir):
            os.makedirs(self.base_download_dir)
        
        # ì‚¬ì´íŠ¸-ì§€ìì²´ëª… ë§¤í•‘ ë¡œë“œ
        self.site_name_mapping = self._load_site_name_mapping()
        
        self.stats = {}
        
    def _create_session(self) -> requests.Session:
        """ì„¸ì…˜ ìƒì„± ë° ê¸°ë³¸ ì„¤ì •"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        return session
    
    def _init_selenium(self):
        """Selenium ì´ˆê¸°í™”"""
        if not SELENIUM_AVAILABLE:
            return
            
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument(f'user-agent={random.choice(self.user_agents)}')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        self.driver = webdriver.Chrome(options=chrome_options)
    
    def _load_site_name_mapping(self) -> Dict[str, str]:
        """ì‚¬ì´íŠ¸-ì§€ìì²´ëª… ë§¤í•‘"""
        return {
            # ì„œìš¸ì‹œ ìì¹˜êµ¬
            'ydp.go.kr': 'ì˜ë“±í¬êµ¬',
            'dongjak.go.kr': 'ë™ì‘êµ¬',
            'assembly.dongjak.go.kr': 'ë™ì‘êµ¬',
            'yscl.go.kr': 'ìš©ì‚°êµ¬',
            'gwangjin.go.kr': 'ê´‘ì§„êµ¬',
            'council.gwangjin.go.kr': 'ê´‘ì§„êµ¬',
            'seocho.go.kr': 'ì„œì´ˆêµ¬',
            'gangdong.go.kr': 'ê°•ë™êµ¬',
            'mapo.seoul.kr': 'ë§ˆí¬êµ¬',
            'council.mapo.seoul.kr': 'ë§ˆí¬êµ¬',
            'ddm.go.kr': 'ë™ëŒ€ë¬¸êµ¬',
            'sb.go.kr': 'ì„±ë¶êµ¬',
            'dobong.go.kr': 'ë„ë´‰êµ¬',
            'nowon.kr': 'ë…¸ì›êµ¬',
            'gangseo.seoul.kr': 'ê°•ì„œêµ¬',
            'ycc.go.kr': 'ì–‘ì²œêµ¬',
            'guro.go.kr': 'êµ¬ë¡œêµ¬',
            'geumcheon.go.kr': 'ê¸ˆì²œêµ¬',
            'songpa.go.kr': 'ì†¡íŒŒêµ¬',
            'gangnam.go.kr': 'ê°•ë‚¨êµ¬',
            'ep.go.kr': 'ì€í‰êµ¬',
            'council.ep.go.kr': 'ì€í‰êµ¬',
            'jongno.go.kr': 'ì¢…ë¡œêµ¬',
            'sd.go.kr': 'ì„±ë™êµ¬',
            'jungnang.go.kr': 'ì¤‘ë‘êµ¬',
            'gangbuk.go.kr': 'ê°•ë¶êµ¬',
            'council.gangbuk.go.kr': 'ê°•ë¶êµ¬',
            'junggu.seoul.kr': 'ì¤‘êµ¬',
            
            # ê²½ê¸°ë„
            'suwon.go.kr': 'ìˆ˜ì›ì‹œ',
            'council.suwon.go.kr': 'ìˆ˜ì›ì‹œ',
            'goyang.go.kr': 'ê³ ì–‘ì‹œ',
            'yongin.go.kr': 'ìš©ì¸ì‹œ',
            'seongnam.go.kr': 'ì„±ë‚¨ì‹œ',
            'bucheon.go.kr': 'ë¶€ì²œì‹œ',
            'ansan.go.kr': 'ì•ˆì‚°ì‹œ',
            'anyang.go.kr': 'ì•ˆì–‘ì‹œ',
            'namyangju.go.kr': 'ë‚¨ì–‘ì£¼ì‹œ',
            'hwaseong.go.kr': 'í™”ì„±ì‹œ',
            'pyeongtaek.go.kr': 'í‰íƒì‹œ',
            'uijeongbu.go.kr': 'ì˜ì •ë¶€ì‹œ',
            'siheung.go.kr': 'ì‹œí¥ì‹œ',
            'gimpo.go.kr': 'ê¹€í¬ì‹œ',
            'gwangju.go.kr': 'ê´‘ì£¼ì‹œ',
            'gwangmyeong.go.kr': 'ê´‘ëª…ì‹œ',
            'gunpo.go.kr': 'êµ°í¬ì‹œ',
            'osan.go.kr': 'ì˜¤ì‚°ì‹œ',
            'icheon.go.kr': 'ì´ì²œì‹œ',
            'yangju.go.kr': 'ì–‘ì£¼ì‹œ',
            'anseong.go.kr': 'ì•ˆì„±ì‹œ',
            'guri.go.kr': 'êµ¬ë¦¬ì‹œ',
            'pocheon.go.kr': 'í¬ì²œì‹œ',
            'uiwang.go.kr': 'ì˜ì™•ì‹œ',
            'hanam.go.kr': 'í•˜ë‚¨ì‹œ',
            'paju.go.kr': 'íŒŒì£¼ì‹œ',
            'yangpyeong.go.kr': 'ì–‘í‰êµ°',
            'yeoju.go.kr': 'ì—¬ì£¼ì‹œ',
            'dongducheon.go.kr': 'ë™ë‘ì²œì‹œ',
            'gapyeong.go.kr': 'ê°€í‰êµ°',
            'yeoncheon.go.kr': 'ì—°ì²œêµ°',
            
            # íŠ¹ìˆ˜ ì‚¬ì´íŠ¸
            'sscf2016.or.kr': 'ì„œì´ˆêµ¬ì¬ë‹¨',
        }
    
    def get_target_months(self) -> List[Tuple[int, int]]:
        """ëŒ€ìƒ ì›” ê³„ì‚°"""
        months = []
        current = self.current_date
        
        for i in range(3):
            months.append((current.year, current.month))
            if current.month == 1:
                current = current.replace(year=current.year - 1, month=12)
            else:
                current = current.replace(month=current.month - 1)
        
        logger.info(f"ëŒ€ìƒ ê¸°ê°„: {months}")
        return months
    
    def is_target_date(self, text: str) -> Tuple[bool, Optional[str]]:
        """ë‚ ì§œ í™•ì¸ - ê°œì„ ëœ ë²„ì „"""
        if not text:
            return False, None
        
        # ê³µë°± ì •ê·œí™”
        text = ' '.join(text.split())
        
        patterns = [
            (r'(\d{4})[\s\-\.ë…„/](\d{1,2})[\s\-\.ì›”]?', 'full'),
            (r'(\d{4})(\d{2})', 'compact'),
            (r'(\d{2})[\s\-\.ë…„](\d{1,2})[\s\-\.ì›”]', 'short'),
        ]
        
        for pattern, format_type in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                try:
                    if format_type == 'short':
                        year = 2000 + int(match[0])
                    else:
                        year = int(match[0])
                    month = int(match[1])
                    
                    if 1 <= month <= 12 and (year, month) in self.target_months:
                        return True, f"{year}ë…„ {month}ì›”"
                except:
                    continue
        
        # í•œê¸€ ì›” ì´ë¦„
        month_map = {
            '1ì›”': 1, '2ì›”': 2, '3ì›”': 3, '4ì›”': 4, '5ì›”': 5, '6ì›”': 6,
            '7ì›”': 7, '8ì›”': 8, '9ì›”': 9, '10ì›”': 10, '11ì›”': 11, '12ì›”': 12,
            'ì¼ì›”': 1, 'ì´ì›”': 2, 'ì‚¼ì›”': 3, 'ì‚¬ì›”': 4, 'ì˜¤ì›”': 5, 'ìœ ì›”': 6,
            'ì¹ ì›”': 7, 'íŒ”ì›”': 8, 'êµ¬ì›”': 9, 'ì‹œì›”': 10, 'ì‹­ì¼ì›”': 11, 'ì‹­ì´ì›”': 12
        }
        
        for month_name, month_num in month_map.items():
            if month_name in text:
                year_match = re.search(r'(\d{4})ë…„?', text)
                year = int(year_match.group(1)) if year_match else self.current_date.year
                
                if (year, month_num) in self.target_months:
                    return True, f"{year}ë…„ {month_num}ì›”"
                if (year - 1, month_num) in self.target_months:
                    return True, f"{year - 1}ë…„ {month_num}ì›”"
        
        return False, None
    
    def get_site_name(self, url: str) -> str:
        """ì‚¬ì´íŠ¸ ì´ë¦„ ì¶”ì¶œ - ê°•í™”ëœ ë²„ì „"""
        domain = urlparse(url).netloc.lower()
        path = urlparse(url).path.lower()
        
        # ì§ì ‘ ë§¤í•‘ ì‹œë„ (ì „ì²´ ë„ë©”ì¸)
        if domain in self.site_name_mapping:
            return self.site_name_mapping[domain]
        
        # ë„ë©”ì¸ ë¶€ë¶„ ë§¤ì¹­
        for key, name in self.site_name_mapping.items():
            if key in domain:
                return name
        
        # council íŠ¹ìˆ˜ ì²˜ë¦¬ (ì˜íšŒ ì‚¬ì´íŠ¸)
        if 'council' in domain:
            domain_parts = domain.split('.')
            if len(domain_parts) >= 3:
                council_site = '.'.join(domain_parts[1:])
                if council_site in self.site_name_mapping:
                    return f"{self.site_name_mapping[council_site]}ì˜íšŒ"
                
                # council.XXX.go.kr íŒ¨í„´
                middle_domain = domain_parts[1]
                for key, name in self.site_name_mapping.items():
                    if middle_domain in key:
                        return f"{name}ì˜íšŒ"
                        
            return 'ì˜íšŒ'
        
        # www íŠ¹ìˆ˜ ì²˜ë¦¬
        if domain.startswith('www.'):
            domain_without_www = domain[4:]
            if domain_without_www in self.site_name_mapping:
                return self.site_name_mapping[domain_without_www]
            
            for key, name in self.site_name_mapping.items():
                if key in domain_without_www:
                    return name
        
        # íŒ¨ìŠ¤ì—ì„œ íŒíŠ¸ ì°¾ê¸°
        if 'council' in path or 'assembly' in path:
            for key, name in self.site_name_mapping.items():
                if key in domain:
                    return f"{name}ì˜íšŒ"
        
        # ìµœí›„ì˜ ìˆ˜ë‹¨: ë„ë©”ì¸ ì²« ë¶€ë¶„
        domain_parts = domain.split('.')
        if len(domain_parts) > 0 and domain_parts[0] != 'www' and len(domain_parts[0]) > 2:
            return domain_parts[0]
        
        # ë‘ ë²ˆì§¸ ë¶€ë¶„ì´ ì˜ë¯¸ìˆëŠ” ê²½ìš° (www.xxx.go.kr)
        if len(domain_parts) > 1 and domain_parts[0] == 'www' and len(domain_parts[1]) > 2:
            return domain_parts[1]
        
        return domain
    
    def build_download_url_variants(self, base_url: str, file_id: str, file_name: str = '') -> List[str]:
        """ë‹¤ì–‘í•œ ë‹¤ìš´ë¡œë“œ URL íŒ¨í„´ ìƒì„±"""
        parsed = urlparse(base_url)
        base = f"{parsed.scheme}://{parsed.netloc}"
        
        # ê°€ëŠ¥í•œ ëª¨ë“  ë‹¤ìš´ë¡œë“œ URL íŒ¨í„´
        patterns = [
            f"{base}/common/download.do?fileId={file_id}",
            f"{base}/common/fileDown.do?fileId={file_id}",
            f"{base}/web/board/BD_fileDownload.do?fileNo={file_id}",
            f"{base}/file/download.do?atchFileId={file_id}",
            f"{base}/attach/download.do?fileSeq={file_id}",
            f"{base}/common/fileDown.do?file_id={file_id}",
            f"{base}/board/file_download.do?idx={file_id}",
            f"{base}/board/download.do?file_seq={file_id}",
            f"{base}/bbs/download.do?atchFileId={file_id}",
            f"{base}/cmm/fms/FileDown.do?atchFileId={file_id}",
            f"{base}/file.do?method=download&fileId={file_id}",
            f"{base}/common/downloadFile.do?fileId={file_id}",
            f"{base}/board/fileDownload.do?fileId={file_id}",
            f"{base}/cmm/fms/getFile.do?atchFileId={file_id}",
            f"{base}/cop/bbs/selectBoardArticleFile.do?atchFileId={file_id}",
            f"{base}/bbs/getBoardFile.do?fileId={file_id}",
            # ë°”ë¡œë³´ê¸° ê´€ë ¨ íŒ¨í„´
            f"{base}/common/viewer.do?fileId={file_id}",
            f"{base}/viewer.do?fileId={file_id}",
            f"{base}/fileViewer.do?fileId={file_id}",
            f"{base}/pdfjs/web/viewer.html?file={file_id}",
        ]
        
        # íŒŒì¼ëª…ì´ ìˆìœ¼ë©´ ì¶”ê°€ íŒ¨í„´
        if file_name:
            encoded_name = quote(file_name)
            patterns.extend([
                f"{base}/download/{encoded_name}",
                f"{base}/files/{encoded_name}",
                f"{base}/attach/{encoded_name}",
                f"{base}/upload/{encoded_name}",
                f"{base}/data/download/{encoded_name}",
            ])
            
            # ë°”ë¡œë³´ê¸° ë§í¬ ì²˜ë¦¬
            if "ë°”ë¡œë³´ê¸°" in file_name:
                file_name_cleaned = file_name.replace("ë°”ë¡œë³´ê¸°", "").strip()
                if file_name_cleaned:
                    encoded_cleaned = quote(file_name_cleaned)
                    patterns.extend([
                        f"{base}/download/{encoded_cleaned}",
                        f"{base}/files/{encoded_cleaned}",
                        f"{base}/attach/{encoded_cleaned}",
                        f"{base}/upload/{encoded_cleaned}",
                    ])
        
        return patterns
    
    def clean_url(self, url: str) -> str:
        """URL ì •ë¦¬ - ê°œí–‰ë¬¸ì ë° ê³µë°± ì œê±°"""
        if not url:
            return url
        
        # ê°œí–‰ë¬¸ì, íƒ­, ê³µë°± ì œê±°
        url = re.sub(r'[\r\n\t]', '', url)
        url = ' '.join(url.split())  # ì¤‘ë³µ ê³µë°± ì œê±°
        url = url.strip()
        
        return url
    
    def extract_all_download_urls(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:
        """ëª¨ë“  ë‹¤ìš´ë¡œë“œ URL ì¶”ì¶œ - ê°•í™” ë²„ì „"""
        download_urls = []
        
        # 1. ë°”ë¡œë³´ê¸°/ë¯¸ë¦¬ë³´ê¸° ë§í¬ (ìµœìš°ì„ )
        for link in soup.find_all(['a', 'button', 'div']):
            text = link.get_text().strip()
            if 'ë°”ë¡œë³´ê¸°' in text or 'ë¯¸ë¦¬ë³´ê¸°' in text:
                href = link.get('href', '')
                onclick = link.get('onclick', '')
                data_link = link.get('data-link', '')
                
                potential_url = href or data_link
                if potential_url:
                    url = self.build_absolute_url(potential_url, base_url)
                    if url and not self.deduplicator.is_duplicate_url(url):
                        # ìƒìœ„ ìš”ì†Œì—ì„œ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ ì‹œë„
                        parent = link.find_parent(['tr', 'li', 'div'])
                        parent_text = parent.get_text().strip() if parent else ""
                        is_target, date_str = self.is_target_date(parent_text)
                        
                        download_urls.append({
                            'url': url,
                            'type': 'preview',
                            'text': text,
                            'date': date_str if is_target else None,
                            'onclick': onclick
                        })
                
                # onclick ì²˜ë¦¬
                if onclick and ('window.open' in onclick or 'download' in onclick.lower()):
                    js_patterns = [
                        r"window\.open\(['\"]([^'\"]+)['\"]",
                        r"download\(['\"]?([^'\"]+)['\"]?",
                        r"fileDown\(['\"]?([^'\"]+)['\"]?",
                    ]
                    
                    for pattern in js_patterns:
                        matches = re.findall(pattern, onclick)
                        for match in matches:
                            preview_url = self.build_absolute_url(match, base_url)
                            if preview_url and not self.deduplicator.is_duplicate_url(preview_url):
                                download_urls.append({
                                    'url': preview_url,
                                    'type': 'preview-onclick',
                                    'text': text,
                                    'onclick': onclick
                                })
        
        # 2. href ê¸°ë°˜ ë§í¬
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            onclick = link.get('onclick', '')
            text = link.get_text().strip()
            title = link.get('title', '')
            
            # íŒŒì¼ í™•ì¥ì ì²´í¬
            file_extensions = ['.pdf', '.xlsx', '.xls', '.hwp', '.doc', '.docx', '.zip', '.csv', '.hwpx']
            is_file_link = any(ext in href.lower() for ext in file_extensions)
            
            # ë‹¤ìš´ë¡œë“œ í‚¤ì›Œë“œ ì²´í¬
            download_keywords = ['download', 'fileDown', 'attachDown', 'file', 'attach',  
                                 'getFile', 'atchFile', 'boardFile', 'bbsFile', 'FileDown']
            is_download_link = any(kw in href.lower() or kw in onclick.lower()  
                                   for kw in download_keywords)
            
            if is_file_link or is_download_link:
                url = self.build_absolute_url(href, base_url)
                if url and not self.deduplicator.is_duplicate_url(url):
                    download_urls.append({
                        'url': url,
                        'type': 'direct',
                        'text': text,
                        'title': title,
                        'onclick': onclick
                    })
        
        # 3. onclick ê¸°ë°˜ ë§í¬ - í™•ì¥ëœ íŒ¨í„´
        for link in soup.find_all(['a', 'button', 'span', 'div'], onclick=True):
            onclick = link.get('onclick', '')
            text = link.get_text().strip()
            
            # JavaScript í•¨ìˆ˜ íŒ¨í„´ë“¤
            js_patterns = [
                r"fn_download\(['\"]?([^'\"]+)['\"]?(?:,\s*['\"]?([^'\"]+)['\"]?)?\)",
                r"fileDownload\(['\"]?([^'\"]+)['\"]?(?:,\s*['\"]?([^'\"]+)['\"]?)?\)",
                r"download\(['\"]?([^'\"]+)['\"]?(?:,\s*['\"]?([^'\"]+)['\"]?)?\)",
                r"attachDown\(['\"]?([^'\"]+)['\"]?(?:,\s*['\"]?([^'\"]+)['\"]?)?\)",
                r"fn_fileDown\(['\"]?([^'\"]+)['\"]?(?:,\s*['\"]?([^'\"]+)['\"]?)?\)",
                r"jsFileDownload\(['\"]?([^'\"]+)['\"]?(?:,\s*['\"]?([^'\"]+)['\"]?)?\)",
                r"getFile\(['\"]?([^'\"]+)['\"]?(?:,\s*['\"]?([^'\"]+)['\"]?)?\)",
                r"file_down\(['\"]?([^'\"]+)['\"]?(?:,\s*['\"]?([^'\"]+)['\"]?)?\)",
                r"fnFileDown\(['\"]?([^'\"]+)['\"]?(?:,\s*['\"]?([^'\"]+)['\"]?)?\)",
                r"boardFileDown\(['\"]?([^'\"]+)['\"]?(?:,\s*['\"]?([^'\"]+)['\"]?)?\)",
                r"filePreview\(['\"]?([^'\"]+)['\"]?(?:,\s*['\"]?([^'\"]+)['\"]?)?\)",
                r"viewer\(['\"]?([^'\"]+)['\"]?(?:,\s*['\"]?([^'\"]+)['\"]?)?\)",
            ]
            
            for pattern in js_patterns:
                matches = re.findall(pattern, onclick)
                for match in matches:
                    file_id = match[0] if isinstance(match, tuple) else match
                    file_name = match[1] if isinstance(match, tuple) and len(match) > 1 else ''
                    
                    # ë‹¤ì¤‘ URL ì‹œë„
                    url_variants = self.build_download_url_variants(base_url, file_id, file_name)
                    
                    for url in url_variants:
                        if not self.deduplicator.is_duplicate_url(url):
                            download_urls.append({
                                'url': url,
                                'type': 'onclick',
                                'text': text or file_name,
                                'file_id': file_id,
                                'file_name': file_name,
                                'variants': url_variants
                            })
                            break
        
        # 4. form ê¸°ë°˜ ë‹¤ìš´ë¡œë“œ
        for form in soup.find_all('form'):
            action = form.get('action', '')
            if 'download' in action.lower() or 'file' in action.lower():
                inputs = form.find_all('input')
                form_data = {}
                for inp in inputs:
                    name = inp.get('name')
                    value = inp.get('value')
                    if name and value:
                        form_data[name] = value
                
                if form_data:
                    url = self.build_absolute_url(action, base_url)
                    if url and not self.deduplicator.is_duplicate_url(url):
                        download_urls.append({
                            'url': url,
                            'type': 'form',
                            'method': form.get('method', 'get'),
                            'data': form_data
                        })
        
        # 5. data-* ì†ì„± ì²´í¬
        data_attrs = ['data-file', 'data-url', 'data-href', 'data-link', 'data-download', 'data-attach']
        for element in soup.find_all():
            for attr in data_attrs:
                if element.has_attr(attr):
                    file_url = element.get(attr)
                    if file_url:
                        url = self.build_absolute_url(file_url, base_url)
                        if url and not self.deduplicator.is_duplicate_url(url):
                            download_urls.append({
                                'url': url,
                                'type': 'data-attr',
                                'text': element.get_text().strip()
                            })
                        break  # ì²« ë²ˆì§¸ ì¼ì¹˜í•˜ëŠ” data-* ì†ì„±ë§Œ ì²˜ë¦¬
        
        # 6. iframe ë‚´ë¶€ íƒìƒ‰
        for iframe in soup.find_all('iframe'):
            src = iframe.get('src')
            if src:
                iframe_url = self.build_absolute_url(src, base_url)
                if iframe_url:
                    try:
                        iframe_response = self.session.get(iframe_url, timeout=10, verify=False)
                        iframe_soup = BeautifulSoup(iframe_response.text, 'html.parser')
                        iframe_links = self.extract_all_download_urls(iframe_soup, iframe_url)
                        download_urls.extend(iframe_links)
                    except:
                        pass
        
        return download_urls
    
    def build_absolute_url(self, url: str, base_url: str) -> Optional[str]:
        """ì ˆëŒ€ URL ìƒì„± - ê°œì„ ëœ ë²„ì „"""
        if not url or url.startswith('#') or url.startswith('javascript'):
            return None
        
        # URL ì •ë¦¬
        url = self.clean_url(url)
        
        # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì²´í¬ (D:/, C:/ ë“±)
        if re.match(r'^[A-Za-z]:[/\\]', url):
            logger.debug(f"ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ë¬´ì‹œ: {url}")
            return None
        
        if url.startswith('http'):
            return url
        
        parsed_base = urlparse(base_url)
        
        if url.startswith('//'):
            return f"{parsed_base.scheme}:{url}"
        
        if url.startswith('/'):
            return f"{parsed_base.scheme}://{parsed_base.netloc}{url}"
        
        # URL ê²°í•© í›„ ì •ê·œí™”
        joined_url = urljoin(base_url, url)
        parsed = urlparse(joined_url)
        normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        if parsed.query:
            normalized += f"?{parsed.query}"
        if parsed.fragment:
            normalized += f"#{parsed.fragment}"
            
        return normalized
    
    def explore_detail_page(self, detail_url: str, base_url: str) -> List[Dict]:
        """ìƒì„¸ í˜ì´ì§€ íƒìƒ‰ - ì¬ì‹œë„ í¬í•¨"""
        download_urls = []
        
        for attempt in range(3):  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
            try:
                headers = {'User-Agent': random.choice(self.user_agents)}
                response = self.session.get(detail_url, timeout=15, verify=False, headers=headers)
                response.encoding = self._detect_encoding(response)
                soup = BeautifulSoup(response.text, 'html.parser')
                
                detail_downloads = self.extract_all_download_urls(soup, detail_url)
                download_urls.extend(detail_downloads)
                break
                
            except Exception as e:
                if attempt < 2:  # ë§ˆì§€ë§‰ ì‹œë„ ì „ê¹Œì§€
                    logger.debug(f"ìƒì„¸ í˜ì´ì§€ ì‹œë„ {attempt+1} ì‹¤íŒ¨: {detail_url}")
                    time.sleep(1)
                else:
                    logger.debug(f"ìƒì„¸ í˜ì´ì§€ ìµœì¢… ì‹¤íŒ¨: {detail_url} - {e}")
        
        return download_urls
    
    def _detect_encoding(self, response: requests.Response) -> str:
        """ì‘ë‹µ ì¸ì½”ë”© ê°ì§€"""
        # 1. Content-Type í—¤ë” í™•ì¸
        content_type = response.headers.get('Content-Type', '').lower()
        charset_match = re.search(r'charset=([^\s;]+)', content_type)
        if charset_match:
            return charset_match.group(1)
            
        # 2. HTML ë©”íƒ€ íƒœê·¸ í™•ì¸
        charset_pattern = re.compile(rb'<meta.*?charset=["\']*([^\s"\'/>]+)', re.I)
        match = charset_pattern.search(response.content)
        if match:
            return match.group(1).decode()
            
        # 3. BOM í™•ì¸
        if response.content.startswith(b'\xef\xbb\xbf'):
            return 'utf-8-sig'
            
        # 4. ìë™ ê°ì§€
        return response.apparent_encoding or 'utf-8'
    
    def find_detail_page_links(self, soup: BeautifulSoup, base_url: str) -> List[Tuple[str, str]]:
        """ìƒì„¸ í˜ì´ì§€ ë§í¬ ì°¾ê¸°"""
        detail_links: List[Tuple[str, str]] = []

        detail_keywords = ['view', 'detail', 'read', 'content', 'article', 'show', 'View', 'Detail', 'ìƒì„¸']

        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            if not href:
                continue

            # href ë˜ëŠ” ë§í¬ í…ìŠ¤íŠ¸ ì²´í¬
            is_detail = any(kw in href for kw in detail_keywords)
            if not is_detail:
                link_text = link.get_text().strip()
                is_detail = any(kw in link_text for kw in ['ìƒì„¸', 'ë³´ê¸°', 'ì¡°íšŒ', 'ë‚´ìš©'])
            
            if is_detail:
                parent = link.find_parent(['tr', 'li', 'div', 'article'])
                context_text = ''
                if parent:
                    context_text = parent.get_text(separator=' ', strip=True)
                else:
                    context_text = link.get_text(separator=' ', strip=True)

                is_target, date_str = self.is_target_date(context_text)
                if is_target:
                    url = self.build_absolute_url(href, base_url)
                    if url:
                        item = (url, date_str or '')
                        if item not in detail_links:
                            detail_links.append(item)

        return detail_links

    def download_file_with_retry(self, file_info: Dict, save_dir: str, max_retries: int = 3) -> bool:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (v5.0: ë°”ë¡œë³´ê¸° ë§í¬ ì²˜ë¦¬ ê°œì„ )"""
        backoffs = [0, 2, 4, 8]  # ì²« ì‹œë„ëŠ” 0ì´ˆ
        variants = file_info.get('variants', [])
        tried_urls: Set[str] = set()

        # ì²« ë²ˆì§¸ëŠ” ì› URL, ì´í›„ variants ì„ì–´ì„œ ì‹œë„
        candidate_rounds: List[List[str]] = []
        primary = file_info.get('url')
        if primary:
            candidate_rounds.append([primary])

        if variants:
            # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
            uniq_variants = [u for u in variants if u and u not in tried_urls]
            candidate_rounds.extend([[u] for u in uniq_variants[:4]])  # ê³¼ë„ ì‹œë„ ë°©ì§€

        # "ë°”ë¡œë³´ê¸°" ë˜ëŠ” "ë¯¸ë¦¬ë³´ê¸°" ê´€ë ¨ íŠ¹ë³„ ì²˜ë¦¬
        original_text = file_info.get('text', '')
        if 'ë°”ë¡œë³´ê¸°' in original_text or 'ë¯¸ë¦¬ë³´ê¸°' in original_text:
            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
            date_part = re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”', original_text)
            if date_part:
                year, month = date_part.groups()
                # íŒŒì¼ëª… êµ¬ì„±
                file_name_guess = f"{year}ë…„ {month}ì›”_ì—…ë¬´ì¶”ì§„ë¹„.pdf"
                file_info['file_name'] = file_name_guess

        tries = 0
        for attempt in range(min(max_retries, len(backoffs))):
            wait = backoffs[attempt] if attempt < len(backoffs) else backoffs[-1]
            if wait:
                time.sleep(wait)

            url_batch = candidate_rounds[attempt] if attempt < len(candidate_rounds) else []
            if not url_batch and primary and primary not in tried_urls:
                url_batch = [primary]

            for url in url_batch:
                if not url or url in tried_urls:
                    continue
                tried_urls.add(url)

                try_info = dict(file_info)
                try_info['url'] = url
                try:
                    if self.download_file(try_info, save_dir):
                        return True
                except requests.exceptions.HTTPError as e:
                    code = getattr(e.response, 'status_code', None)
                    if code and 400 <= code < 500 and code != 429:
                        logger.debug(f"HTTP {code}ë¡œ ì¤‘ë‹¨: {url}")
                        # 4xx (429 ì œì™¸)ëŠ” ì¦‰ì‹œ ë‹¤ìŒ ë³€í˜• ì‹œë„
                        continue
                    # 5xx ë˜ëŠ” 429ëŠ” ë‹¤ìŒ attemptë¡œ ë°±ì˜¤í”„
                except (requests.exceptions.Timeout,
                        requests.exceptions.SSLError,
                        requests.exceptions.ConnectionError) as e:
                    logger.debug(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì¬ì‹œë„ ì˜ˆì •: {e}")
                    # ë‹¤ìŒ attempt
                except Exception as e:
                    logger.debug(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜(ê³„ì† ì§„í–‰): {e}")

                tries += 1

        return False

    def decode_filename(self, text: str) -> str:
        """íŒŒì¼ëª… ë””ì½”ë”© - ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„ (v5.0: ê°œì„ )"""
        if not text:
            return text

        # ì´ë¯¸ í•œê¸€/ìœ ë‹ˆì½”ë“œë¡œ ì •ìƒì¼ ìˆ˜ ìˆìŒ
        try:
            if re.search(r'[\uAC00-\uD7A3]', text):  # í•œê¸€ í¬í•¨ í™•ì¸
                return text  # ì´ë¯¸ í•œê¸€ì´ ì •ìƒì ìœ¼ë¡œ í¬í•¨ëœ ê²½ìš°
            
            text.encode('ascii')  # ASCIIë¡œ ì¸ì½”ë”© ì‹œë„
            # ASCIIë¡œ ì¸ì½”ë”© ê°€ëŠ¥í•˜ë©´ ì´ê²ƒì€ ì¸ì½”ë”© ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ
        except UnicodeEncodeError:
            # ì´ë¯¸ ìœ ë‹ˆì½”ë“œë¡œ ì •ìƒì ì¸ ê²½ìš°
            return text

        # URL ì¸ì½”ë”© í™•ì¸
        if '%' in text:
            try:
                decoded = unquote(text)
                if decoded != text:
                    return decoded
            except Exception:
                pass

        # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
        encodings = ['utf-8', 'euc-kr', 'cp949', 'iso-8859-1', 'latin-1']
        for encoding in encodings:
            try:
                # latin-1ë¡œ ë°”ì´íŠ¸ë¡œ ë³€í™˜ í›„ ë‹¤ì‹œ ëª©í‘œ ì¸ì½”ë”©ìœ¼ë¡œ ë””ì½”ë”©
                decoded = text.encode('latin-1').decode(encoding)
                
                # ì„±ê³µì ì¸ ë””ì½”ë”© í™•ì¸ (í•œê¸€ í¬í•¨ ì—¬ë¶€)
                if re.search(r'[\uAC00-\uD7A3]', decoded):
                    return decoded
            except Exception:
                continue

        # ë§ˆì§€ë§‰ ì‹œë„: ë‹¨ìˆœ URL ë””ì½”ë”©
        try:
            return unquote(text)
        except:
            pass

        return text

    def download_file(self, file_info: Dict, save_dir: str) -> bool:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        url = file_info['url']
        method = file_info.get('method', 'get').lower()
        data = file_info.get('data')

        headers = {
            'Referer': url,
            'Accept': '*/*',
            'User-Agent': random.choice(self.user_agents)
        }

        # ì‹¤ì œ ìš”ì²­
        try:
            if method == 'post' and data:
                resp = self.session.post(url, data=data, headers=headers,
                                         timeout=30, stream=True, verify=False, allow_redirects=True)
            else:
                resp = self.session.get(url, headers=headers,
                                        timeout=30, stream=True, verify=False, allow_redirects=True)

            resp.raise_for_status()
        except Exception as e:
            logger.debug(f"ë‹¤ìš´ë¡œë“œ ìš”ì²­ ì‹¤íŒ¨: {url} - {e}")
            return False

        # ì½˜í…ì¸  íƒ€ì… ê²€ì‚¬
        content_type = resp.headers.get('Content-Type', '').lower()
        if 'text/html' in content_type and 'attachment' not in resp.headers.get('Content-Disposition', ''):
            # HTML ì‘ë‹µì´ì§€ë§Œ ë°”ë¡œë³´ê¸° ë§í¬ì¸ ê²½ìš° PDF ë³€í™˜ ì‹œë„
            if 'ë°”ë¡œë³´ê¸°' in file_info.get('text', '') or 'ë¯¸ë¦¬ë³´ê¸°' in file_info.get('text', ''):
                if self.use_selenium and SELENIUM_AVAILABLE and self.driver:
                    return self._download_preview_with_selenium(url, file_info, save_dir)
            logger.debug(f"HTML ì‘ë‹µ(íŒŒì¼ ì•„ë‹˜) ê±´ë„ˆëœ€: {url}")
            return False

        filename = self.extract_filename(resp, file_info)

        # íŒŒì¼ëª… ì¤‘ë³µ ì²´í¬
        if self.deduplicator.is_duplicate_filename(filename, url):
            logger.debug(f"ì¤‘ë³µ íŒŒì¼ëª… ê±´ë„ˆëœ€: {filename}")
            return False

        save_path = os.path.join(save_dir, filename)
        save_path = self.get_unique_filepath(save_path)

        # ì €ì¥
        with open(save_path, 'wb') as f:
            for chunk in resp.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        # ë‚´ìš© ì¤‘ë³µ ì²´í¬(í•´ì‹œ)
        if self.deduplicator.is_duplicate_content(save_path):
            logger.info(f"ì¤‘ë³µ ë‚´ìš© ì‚­ì œ: {filename}")
            os.remove(save_path)
            return False

        size = os.path.getsize(save_path)
        if size < 100:
            logger.warning(f"íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŒ ({size} bytes): {filename}")
            os.remove(save_path)
            return False

        logger.info(f"âœ“ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {filename} ({size:,} bytes)")
        return True
    
    def _download_preview_with_selenium(self, url: str, file_info: Dict, save_dir: str) -> bool:
        """Seleniumìœ¼ë¡œ ë°”ë¡œë³´ê¸°/ë¯¸ë¦¬ë³´ê¸° ë‹¤ìš´ë¡œë“œ"""
        if not self.driver:
            return False
            
        try:
            self.driver.get(url)
            time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # PDF ë‚´ìš© í™•ì¸ (iframe ë˜ëŠ” embed ìš”ì†Œ)
            pdf_elements = self.driver.find_elements(By.TAG_NAME, "iframe") + \
                           self.driver.find_elements(By.TAG_NAME, "embed") + \
                           self.driver.find_elements(By.TAG_NAME, "object")
                           
            if not pdf_elements:
                return False
                
            pdf_src = None
            for elem in pdf_elements:
                src = elem.get_attribute("src") or elem.get_attribute("data")
                if src and ('.pdf' in src or 'viewer' in src):
                    pdf_src = src
                    break
                    
            if not pdf_src:
                return False
                
            # PDF URL ì¶”ì¶œ
            pdf_url = pdf_src
            if '?file=' in pdf_src:
                pdf_url = re.search(r'\?file=([^&]+)', pdf_src).group(1)
                
            if not pdf_url:
                return False
                
            # íŒŒì¼ëª… ìƒì„±
            date_str = file_info.get('date', '')
            if not date_str:
                text = file_info.get('text', '')
                date_match = re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”', text)
                if date_match:
                    date_str = f"{date_match.group(1)}ë…„ {date_match.group(2)}ì›”"
                    
            filename = date_str + "_ì—…ë¬´ì¶”ì§„ë¹„.pdf" if date_str else "ì—…ë¬´ì¶”ì§„ë¹„.pdf"
            
            # ì›ë³¸ PDF ë‹¤ìš´ë¡œë“œ
            headers = {
                'Referer': url,
                'User-Agent': self.driver.execute_script("return navigator.userAgent"),
            }
            
            resp = self.session.get(pdf_url, headers=headers, timeout=30, 
                                  stream=True, verify=False, allow_redirects=True)
                                  
            save_path = os.path.join(save_dir, filename)
            save_path = self.get_unique_filepath(save_path)
            
            with open(save_path, 'wb') as f:
                for chunk in resp.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        
            size = os.path.getsize(save_path)
            if size < 100:
                logger.warning(f"ë°”ë¡œë³´ê¸° PDFê°€ ë„ˆë¬´ ì‘ìŒ ({size} bytes): {filename}")
                os.remove(save_path)
                return False
                
            logger.info(f"âœ“ ë°”ë¡œë³´ê¸° ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {filename} ({size:,} bytes)")
            return True
            
        except Exception as e:
            logger.debug(f"Selenium ë°”ë¡œë³´ê¸° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def extract_filename(self, response: requests.Response, file_info: Dict) -> str:
        """íŒŒì¼ëª… ì¶”ì¶œ - ì¸ì½”ë”©/í™•ì¥ì ì²˜ë¦¬ ê°œì„ (v5.0)"""
        filename: Optional[str] = None

        # 1) Content-Disposition
        cd = response.headers.get('Content-Disposition', '')
        if cd:
            # RFC 5987
            m = re.findall(r"filename\*=UTF-8''([^;]+)", cd)
            if m:
                filename = unquote(m[0])

            if not filename:
                m = re.findall(r"filename\*=([^']+)''([^;]+)", cd)
                if m:
                    _, enc_name = m[0]
                    try:
                        filename = unquote(enc_name)
                    except Exception:
                        filename = enc_name

            if not filename:
                m = re.findall(r'filename[^;=\n]*=(([\'"]).*?\2|[^;\n]*)', cd)
                if m:
                    raw = m[0][0].strip('"\'')

                    # URL ë””ì½”ë”© + ì¸ì½”ë”© êµì •
                    try:
                        decoded = unquote(raw)
                        filename = decoded if decoded else raw
                    except Exception:
                        filename = raw

                    filename = self.decode_filename(filename)

        # 2) file_info íŒíŠ¸ - ë‚ ì§œ ì •ë³´ í™œìš©
        if not filename or len(filename) < 2:
            date_info = file_info.get('date', '')
            text_info = file_info.get('text', '').strip()
            
            if date_info and date_info not in text_info:
                text_info = f"{date_info}_{text_info}"
                
            if text_info:
                filename = self.decode_filename(text_info)
            else:
                for k in ('file_name', 'title'):
                    v = file_info.get(k)
                    if v:
                        filename = self.decode_filename(v.strip())
                        if date_info and date_info not in filename:
                            filename = f"{date_info}_{filename}"
                        break

        # 3) URLì—ì„œ ì¶”ì¶œ
        if not filename or len(filename) < 2:
            url_path = urlparse(file_info['url']).path
            base = os.path.basename(url_path)
            if base and len(base) > 2:
                filename = self.decode_filename(unquote(base))

        # 4) ê¸°ë³¸ê°’
        if not filename or len(filename) < 2:
            site_name = file_info.get('site_name', '')
            date_info = file_info.get('date', '')
            if site_name and date_info:
                filename = f"{site_name}_{date_info}_ì—…ë¬´ì¶”ì§„ë¹„"
            else:
                filename = f"file_{int(time.time())}_{random.randint(1000, 9999)}"

        # íŒŒì¼ëª… ì •ë¦¬
        filename = re.sub(r'[<>:"/\\|?*\x00-\x1f]', '_', filename).strip()
        
        # 'ë°”ë¡œë³´ê¸°' ë˜ëŠ” 'ë¯¸ë¦¬ë³´ê¸°' í…ìŠ¤íŠ¸ ì œê±°
        filename = re.sub(r'ë°”ë¡œë³´ê¸°|ë¯¸ë¦¬ë³´ê¸°', '', filename).strip()
        filename = re.sub(r'__+', '_', filename)  # ì¤‘ë³µ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
        
        # ë„ˆë¬´ ê¸´ íŒŒì¼ëª… ì²˜ë¦¬ (í™•ì¥ì ë³´ì¡´ ì‹œë„)
        if len(filename) > 200:
            name_part = filename[:190]
            ext_match = re.search(r'\.[A-Za-z0-9]+$', filename)
            if ext_match:
                filename = name_part + ext_match.group()
            else:
                filename = name_part

        # í™•ì¥ì í™•ì¸/ì¶”ê°€
        valid_exts = ['.pdf', '.xlsx', '.xls', '.hwp', '.hwpx', '.doc', '.docx', '.zip', '.csv', '.ppt', '.pptx']
        if not any(filename.lower().endswith(ext) for ext in valid_exts):
            ctype = response.headers.get('Content-Type', '').lower()
            ext_map = {
                'pdf': '.pdf',
                'excel': '.xlsx',
                'spreadsheet': '.xlsx',
                'sheet': '.xlsx',
                'hwp': '.hwp',
                'msword': '.doc',
                'wordprocessing': '.docx',
                'zip': '.zip',
                'csv': '.csv',
                'presentation': '.pptx'
            }
            appended = False
            for key, ext in ext_map.items():
                if key in ctype:
                    filename += ext
                    appended = True
                    break

            if not appended:
                # URL íŒíŠ¸
                url_lower = file_info['url'].lower()
                for ext in valid_exts:
                    if ext in url_lower:
                        filename += ext
                        break
                        
                # ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ PDF í™•ì¥ì ì¶”ê°€
                if not any(filename.lower().endswith(ext) for ext in valid_exts):
                    filename += '.pdf'

        return filename

    def get_unique_filepath(self, filepath: str) -> str:
        """ê³ ìœ í•œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        if not os.path.exists(filepath):
            return filepath
        base, ext = os.path.splitext(filepath)
        i = 1
        candidate = filepath
        while os.path.exists(candidate):
            candidate = f"{base}_{i}{ext}"
            i += 1
        return candidate

    def process_site_with_selenium(self, url: str) -> List[Dict]:
        """Seleniumìœ¼ë¡œ ë™ì  í˜ì´ì§€ì—ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬ ìˆ˜ì§‘"""
        results: List[Dict] = []
        if not self.driver:
            return results

        try:
            self.driver.get(url)
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            # ìŠ¤í¬ë¡¤ ë‹¤ìš´ìœ¼ë¡œ ë™ì  ë¡œë”© ìœ ë„
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            for _ in range(3):
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1.5)
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height
                
            # ë”ë³´ê¸°/ì¶”ê°€ ë²„íŠ¼ ì°¾ê¸° ì‹œë„
            more_button_candidates = [
                "//button[contains(text(), 'ë”ë³´ê¸°')]",
                "//a[contains(text(), 'ë”ë³´ê¸°')]",
                "//button[contains(@class, 'more')]",
                "//a[contains(@class, 'more')]",
                "//button[contains(@class, 'load-more')]"
            ]
            
            for xpath in more_button_candidates:
                try:
                    buttons = self.driver.find_elements(By.XPATH, xpath)
                    for button in buttons:
                        if button.is_displayed():
                            button.click()
                            time.sleep(1.5)
                except:
                    continue

            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            results = self.extract_all_download_urls(soup, url)
        except Exception as e:
            logger.debug(f"Selenium ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

        return results

    def process_site(self, url: str) -> Dict:
        """ì‚¬ì´íŠ¸ ì²˜ë¦¬"""
        site_name = self.get_site_name(url)
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“ ì²˜ë¦¬ ì¤‘: {site_name}")
        logger.info(f"ğŸ”— URL: {url}")
        logger.info(f"{'='*60}")

        stats = {
            'site_name': site_name,
            'url': url,
            'total_links': 0,
            'download_candidates': 0,
            'detail_pages': 0,
            'target_files': 0,
            'downloaded': 0,
            'failed': 0,
            'duplicates_removed': 0,
            'errors': []
        }

        site_dir = os.path.join(self.base_download_dir, site_name)
        os.makedirs(site_dir, exist_ok=True)

        try:
            headers = {'User-Agent': random.choice(self.user_agents)}
            resp = self.session.get(url, timeout=30, verify=False, headers=headers)
            resp.encoding = self._detect_encoding(resp)
            soup = BeautifulSoup(resp.text, 'html.parser')

            all_links = soup.find_all('a')
            stats['total_links'] = len(all_links)
            logger.info(f"ğŸ“Š ì „ì²´ ë§í¬ ìˆ˜: {stats['total_links']:,}ê°œ")

            download_urls = self.extract_all_download_urls(soup, url)
            
            # Selenium ë³´ì¡°
            if self.use_selenium and SELENIUM_AVAILABLE and self.driver:
                logger.info("ğŸ¤– Seleniumìœ¼ë¡œ ë™ì  ì½˜í…ì¸  í™•ì¸ ì¤‘...")
                selenium_urls = self.process_site_with_selenium(url)
                # URL ì¤‘ë³µ ì œê±°
                for su in selenium_urls:
                    if not any(du['url'] == su['url'] for du in download_urls):
                        download_urls.append(su)
            
            stats['download_candidates'] = len(download_urls)
            logger.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ í›„ë³´: {stats['download_candidates']}ê°œ")

            detail_links = self.find_detail_page_links(soup, url)
            stats['detail_pages'] = len(detail_links)
            logger.info(f"ğŸ” ìƒì„¸ í˜ì´ì§€: {stats['detail_pages']}ê°œ íƒìƒ‰ ì¤‘...")

            # ìƒì„¸ í˜ì´ì§€ íƒìƒ‰ (ìµœëŒ€ 100ê°œ)
            for durl, date_str in detail_links[:100]:
                for dl in self.explore_detail_page(durl, url):
                    dl['date'] = date_str
                    dl['site_name'] = site_name  # ì‚¬ì´íŠ¸ëª… ì¶”ê°€
                    download_urls.append(dl)
                time.sleep(random.uniform(0.2, 0.5))

            # ê¸°ê°„ í•„í„°ë§
            filtered: List[Dict] = []
            for info in download_urls:
                if info.get('date'):
                    info['site_name'] = site_name  # ì‚¬ì´íŠ¸ëª… ì¶”ê°€
                    filtered.append(info)
                    continue

                text_to_check = f"{info.get('text', '')} {info.get('title', '')}"
                is_target, date_str = self.is_target_date(text_to_check)
                if is_target:
                    info['date'] = date_str
                    info['site_name'] = site_name  # ì‚¬ì´íŠ¸ëª… ì¶”ê°€
                    filtered.append(info)

            stats['target_files'] = len(filtered)
            logger.info(f"ğŸ¯ ëŒ€ìƒ íŒŒì¼: {stats['target_files']}ê°œ")

            if stats['target_files'] == 0:
                logger.warning(f"âš ï¸  {site_name}: ëŒ€ìƒ ê¸°ê°„ì˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # ì§„í–‰ í‘œì‹œ
            if TQDM_AVAILABLE:
                pbar = tqdm(total=stats['target_files'], desc=f"{site_name} ë‹¤ìš´ë¡œë“œ", 
                            unit="file", leave=False)
            
            for idx, info in enumerate(filtered, 1):
                try:
                    # íŒŒì¼ëª… í”„ë¦¬í”½ìŠ¤ì— ë‚ ì§œ í‘œì‹œ
                    if info.get('date'):
                        original_text = info.get('text', '')
                        if original_text and info['date'] not in original_text:
                            info['text'] = f"{info['date']}_{original_text}"

                    logger.info(f"â¬ [{idx}/{stats['target_files']}] ë‹¤ìš´ë¡œë“œ ì‹œë„: {info.get('text', 'unknown')[:60]}")
                    ok = self.download_file_with_retry(info, site_dir, max_retries=4)
                    if ok:
                        stats['downloaded'] += 1
                    else:
                        stats['failed'] += 1
                        stats['errors'].append(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {info.get('text', 'unknown')[:60]}")

                    # ì§„í–‰ í‘œì‹œ ì—…ë°ì´íŠ¸
                    if TQDM_AVAILABLE:
                        pbar.update(1)
                        
                    time.sleep(random.uniform(0.25, 0.6))
                except Exception as e:
                    stats['failed'] += 1
                    em = f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)[:120]}"
                    stats['errors'].append(em)
                    logger.error(f"âŒ {em}")
                    
                    # ì§„í–‰ í‘œì‹œ ì—…ë°ì´íŠ¸
                    if TQDM_AVAILABLE:
                        pbar.update(1)
            
            # ì§„í–‰ í‘œì‹œ ì¢…ë£Œ
            if TQDM_AVAILABLE:
                pbar.close()

            initial_candidates = stats['download_candidates'] + stats['detail_pages']
            stats['duplicates_removed'] = max(0, initial_candidates - stats['target_files'])

        except requests.exceptions.Timeout:
            em = "í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ"
            stats['errors'].append(em)
            logger.error(f"âŒ {site_name}: {em}")
        except requests.exceptions.ConnectionError:
            em = "ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜"
            stats['errors'].append(em)
            logger.error(f"âŒ {site_name}: {em}")
        except Exception as e:
            em = f"ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)[:200]}"
            stats['errors'].append(em)
            logger.error(f"âŒ {site_name}: {em}")

        if stats['downloaded'] > 0:
            logger.info(f"âœ… {site_name} ì™„ë£Œ: {stats['downloaded']}/{stats['target_files']}ê°œ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
        elif stats['target_files'] > 0:
            logger.warning(f"âš ï¸  {site_name}: {stats['target_files']}ê°œ íŒŒì¼ ë°œê²¬í–ˆìœ¼ë‚˜ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        else:
            logger.info(f"â„¹ï¸  {site_name}: ëŒ€ìƒ íŒŒì¼ ì—†ìŒ")

        return stats
        
    def process_sites_parallel(self, urls: List[str]) -> List[Dict]:
        """ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ë™ì‹œ ì²˜ë¦¬"""
        all_stats = []
        
        if self.max_workers <= 1:
            # ìˆœì°¨ ì²˜ë¦¬
            for i, url in enumerate(urls, 1):
                logger.info(f"\n{'â–¶'*3} ì§„í–‰: {i}/{len(urls)} ({i/len(urls)*100:.1f}%)")
                stats = self.process_site(url)
                all_stats.append(stats)
                self.stats[stats['site_name']] = stats
                
                if i < len(urls):
                    time.sleep(random.uniform(1.0, 2.0))
            return all_stats
        
        # ë³‘ë ¬ ì²˜ë¦¬
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # Seleniumì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° í”„ë¡œì„¸ìŠ¤ í’€ì˜ ê° ì›Œì»¤ì—ê²Œ ì•Œë¦¼
            if self.use_selenium:
                # ê° í”„ë¡œì„¸ìŠ¤ë§ˆë‹¤ ë…ë¦½ì ì¸ Selenium ì„¸ì…˜ì„ ìƒì„±í•´ì•¼ í•¨
                # ì´ë¥¼ ìœ„í•œ ì„¤ì • ì „ë‹¬ (ì§ì ‘ ì‹¤í–‰ ì‹œì—ëŠ” ì²˜ë¦¬ í•„ìš”)
                pass
                
            # ì§„í–‰ í‘œì‹œ
            if TQDM_AVAILABLE:
                results = list(tqdm(
                    executor.map(self.process_site, urls),
                    total=len(urls),
                    desc="ì‚¬ì´íŠ¸ ì²˜ë¦¬ ì¤‘",
                    unit="site"
                ))
            else:
                # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
                futures = [executor.submit(self.process_site, url) for url in urls]
                
                # ê²°ê³¼ ìˆ˜ì§‘
                results = []
                for i, future in enumerate(concurrent.futures.as_completed(futures)):
                    try:
                        stats = future.result()
                        results.append(stats)
                        logger.info(f"ì™„ë£Œ: {i+1}/{len(urls)} - {stats['site_name']}")
                    except Exception as e:
                        logger.error(f"ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                
            all_stats = results
            
            # í†µê³„ ì €ì¥
            for stats in all_stats:
                self.stats[stats['site_name']] = stats
                
        return all_stats

    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ ì§€ìì²´ ì—…ë¬´ì¶”ì§„ë¹„ íŒŒì¼ ë‹¤ìš´ë¡œë” v5.0 ì‹œì‘")
        logger.info(f"{'='*80}")
        logger.info(f"â° ì‹¤í–‰ ì‹œê°: {self.current_date.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"ğŸ“… ëŒ€ìƒ ì›”: {', '.join([f'{y}ë…„ {m}ì›”' for y, m in self.target_months])}")
        logger.info(f"ğŸ§µ ë³‘ë ¬ ì²˜ë¦¬: {self.max_workers}ê°œ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš©")

        url_file = 'urls.txt'
        if os.path.exists('urls_test.txt'):
            url_file = 'urls_test.txt'
            logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: urls_test.txt ì‚¬ìš©")

        if not os.path.exists(url_file):
            logger.error(f"âŒ {url_file} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        with open(url_file, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]

        urls = list(dict.fromkeys(urls))  # dedup
        logger.info(f"ğŸ“‹ ì²˜ë¦¬í•  ì‚¬ì´íŠ¸ ìˆ˜: {len(urls)}ê°œ")
        logger.info(f"{'='*80}\n")

        start = time.time()

        # ë³‘ë ¬ ì²˜ë¦¬
        all_stats = self.process_sites_parallel(urls)

        if self.driver:
            self.driver.quit()

        elapsed = time.time() - start
        self.generate_report(all_stats, elapsed)

    def generate_report(self, all_stats: List[Dict], elapsed_time: float):
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        report: List[str] = []
        report.append("=" * 80)
        report.append("ğŸ“Š ì§€ìì²´ ì—…ë¬´ì¶”ì§„ë¹„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ê²°ê³¼ ë³´ê³ ì„œ v5.0")
        report.append("=" * 80)
        report.append(f"â° ì‹¤í–‰ ì‹œê°„: {self.current_date.strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"â±ï¸  ì†Œìš” ì‹œê°„: {int(elapsed_time // 60)}ë¶„ {int(elapsed_time % 60)}ì´ˆ")
        report.append(f"ğŸ“… ëŒ€ìƒ ê¸°ê°„: {', '.join([f'{y}ë…„ {m}ì›”' for y, m in self.target_months])}")
        report.append(f"ğŸ¢ ì²˜ë¦¬ ì‚¬ì´íŠ¸: {len(all_stats)}ê°œ")
        report.append(f"ğŸ§µ ë³‘ë ¬ ì²˜ë¦¬: {self.max_workers}ê°œ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš©")
        report.append("")

        total_links = sum(s['total_links'] for s in all_stats)
        total_candidates = sum(s['download_candidates'] for s in all_stats)
        total_detail = sum(s['detail_pages'] for s in all_stats)
        total_target = sum(s['target_files'] for s in all_stats)
        total_downloaded = sum(s['downloaded'] for s in all_stats)
        total_failed = sum(s['failed'] for s in all_stats)
        total_duplicates = sum(s['duplicates_removed'] for s in all_stats)

        report.append("=" * 80)
        report.append("ğŸ“ˆ ì „ì²´ ê²°ê³¼ ìš”ì•½")
        report.append("=" * 80)
        report.append(f"  ğŸ”— ì „ì²´ ë§í¬ ìˆ˜: {total_links:,}ê°œ")
        report.append(f"  ğŸ“¥ ë‹¤ìš´ë¡œë“œ í›„ë³´: {total_candidates:,}ê°œ")
        report.append(f"  ğŸ” íƒìƒ‰í•œ ìƒì„¸í˜ì´ì§€: {total_detail:,}ê°œ")
        report.append(f"  ğŸ—‘ï¸  ì¤‘ë³µ ì œê±°: {total_duplicates:,}ê°œ")
        report.append(f"  ğŸ¯ ëŒ€ìƒ íŒŒì¼: {total_target:,}ê°œ")
        report.append(f"  âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {total_downloaded:,}ê°œ")
        report.append(f"  âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {total_failed:,}ê°œ")
        if total_target > 0:
            success_rate = total_downloaded / total_target * 100
            report.append(f"  ğŸ“Š ì„±ê³µë¥ : {success_rate:.1f}%")
        report.append("")

        dedup_stats = self.deduplicator.get_stats()
        report.append("=" * 80)
        report.append("ğŸ—‚ï¸  ì¤‘ë³µ ì œê±° ìƒì„¸ í†µê³„")
        report.append("=" * 80)
        report.append(f"  â€¢ ê³ ìœ  URL: {dedup_stats['unique_urls']:,}ê°œ")
        report.append(f"  â€¢ ê³ ìœ  íŒŒì¼ëª…: {dedup_stats['unique_filenames']:,}ê°œ")
        report.append(f"  â€¢ ê³ ìœ  íŒŒì¼ ë‚´ìš©: {dedup_stats['unique_contents']:,}ê°œ")
        report.append("")

        report.append("=" * 80)
        report.append("ğŸ¢ ì‚¬ì´íŠ¸ë³„ ìƒì„¸ ê²°ê³¼")
        report.append("=" * 80)

        successful = [s for s in all_stats if s['downloaded'] > 0]
        if successful:
            report.append(f"\nâœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ ì‚¬ì´íŠ¸ ({len(successful)}ê°œ)")
            report.append("-" * 80)
            for st in sorted(successful, key=lambda x: x['downloaded'], reverse=True):
                rate = (st['downloaded'] / st['target_files'] * 100) if st['target_files'] > 0 else 0
                report.append(f"\n  ğŸ“ {st['site_name']}: {st['downloaded']}/{st['target_files']}ê°œ ({rate:.1f}%)")
                report.append(f"    URL: {st['url']}")
                report.append(f"    ì „ì²´ë§í¬: {st['total_links']:,} | í›„ë³´: {st['download_candidates']:,} | ìƒì„¸: {st['detail_pages']:,}")
                if st['failed'] > 0:
                    report.append(f"    âš ï¸  ì‹¤íŒ¨: {st['failed']}ê°œ")

        partial_failed = [s for s in all_stats if 0 < s['downloaded'] < s['target_files']]
        if partial_failed:
            report.append(f"\nâš ï¸  ë¶€ë¶„ ì‹¤íŒ¨ ì‚¬ì´íŠ¸ ({len(partial_failed)}ê°œ)")
            report.append("-" * 80)
            for st in partial_failed:
                report.append(f"\n  ğŸ“ {st['site_name']}: {st['downloaded']}/{st['target_files']}ê°œ")
                report.append(f"    URL: {st['url']}")
                if st['errors']:
                    report.append(f"    ì˜¤ë¥˜: {st['errors'][0][:120]}")

        failed = [s for s in all_stats if s['downloaded'] == 0 and s['target_files'] > 0]
        if failed:
            report.append(f"\nâŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‚¬ì´íŠ¸ ({len(failed)}ê°œ)")
            report.append("-" * 80)
            for st in failed:
                report.append(f"\n  ğŸ“ {st['site_name']}: {st['target_files']}ê°œ íŒŒì¼ ëª¨ë‘ ì‹¤íŒ¨")
                report.append(f"    URL: {st['url']}")
                if st['errors']:
                    report.append(f"    ì˜¤ë¥˜: {st['errors'][0][:150]}")

        no_files = [s for s in all_stats if s['target_files'] == 0]
        if no_files:
            report.append(f"\nâ„¹ï¸  ëŒ€ìƒ íŒŒì¼ ì—†ìŒ ({len(no_files)}ê°œ)")
            report.append("-" * 80)
            for st in no_files:
                report.append(f"  â€¢ {st['site_name']} (ë§í¬: {st['total_links']:,}ê°œ)")

        report.append("")
        report.append("=" * 80)
        report.append("ğŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        report.append(f"ğŸ“ ê²°ê³¼ í´ë”: {self.base_download_dir}")
        report.append("=" * 80)

        report_text = '\n'.join(report)
        print("\n" + report_text)

        # ì‚°ì¶œë¬¼ ì €ì¥
        report_file = os.path.join(self.base_download_dir, 'download_report.txt')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_text)

        json_file = os.path.join(self.base_download_dir, 'download_stats.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'execution_time': self.current_date.strftime('%Y-%m-%d %H:%M:%S'),
                'elapsed_seconds': int(elapsed_time),
                'summary': {
                    'total_sites': len(all_stats),
                    'total_links': total_links,
                    'download_candidates': total_candidates,
                    'detail_pages': total_detail,
                    'duplicates_removed': total_duplicates,
                    'target_files': total_target,
                    'downloaded': total_downloaded,
                    'failed': total_failed,
                    'success_rate': round(total_downloaded / total_target * 100, 1) if total_target > 0 else 0
                },
                'deduplication': dedup_stats,
                'sites': all_stats
            }, f, ensure_ascii=False, indent=2)

        csv_file = os.path.join(self.base_download_dir, 'download_summary.csv')
        with open(csv_file, 'w', encoding='utf-8-sig') as f:
            f.write("ì‚¬ì´íŠ¸ëª…,URL,ì „ì²´ë§í¬,ë‹¤ìš´ë¡œë“œí›„ë³´,ìƒì„¸í˜ì´ì§€,ì¤‘ë³µì œê±°,ëŒ€ìƒíŒŒì¼,ë‹¤ìš´ë¡œë“œì„±ê³µ,ë‹¤ìš´ë¡œë“œì‹¤íŒ¨,ì„±ê³µë¥ \n")
            for st in all_stats:
                rate = (st['downloaded'] / st['target_files'] * 100) if st['target_files'] > 0 else 0
                f.write(
                    f"{st['site_name']},{st['url']},{st['total_links']},"
                    f"{st['download_candidates']},{st['detail_pages']},{st['duplicates_removed']},"
                    f"{st['target_files']},{st['downloaded']},{st['failed']},{rate:.1f}%\n"
                )

        logger.info(f"\nğŸ“„ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ:")
        logger.info(f"  â€¢ í…ìŠ¤íŠ¸: {report_file}")
        logger.info(f"  â€¢ JSON: {json_file}")
        logger.info(f"  â€¢ CSV: {csv_file}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description='ì§€ìì²´ ì—…ë¬´ì¶”ì§„ë¹„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ v5.0',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:
  python %(prog)s                 # ê¸°ë³¸ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬)
  python %(prog)s --workers 8       # 8ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ ë³‘ë ¬ ì²˜ë¦¬
  python %(prog)s --selenium        # Selenium ì‚¬ìš© (ë™ì  í˜ì´ì§€)
  python %(prog)s --test            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ 
  python %(prog)s --selenium --test # Selenium + í…ŒìŠ¤íŠ¸ ëª¨ë“œ
  python %(prog)s --debug           # ë””ë²„ê·¸
        """
    )
    parser.add_argument('--selenium', action='store_true', help='Selenium ì‚¬ìš© (ë™ì  í˜ì´ì§€ ì²˜ë¦¬)')
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ: urls_test.txtê°€ ìˆì„ ë•Œ ìš°ì„  ì‚¬ìš©')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥ (DEBUG ë ˆë²¨)')
    parser.add_argument('--workers', type=int, default=4, help='ë³‘ë ¬ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜ (ê¸°ë³¸: 4)')
    parser.add_argument('--outdir', type=str, default='pdf_data', help='ê²°ê³¼ ì €ì¥ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: pdf_data)')

    args = parser.parse_args()

    # ë¡œê·¸ ë ˆë²¨
    if args.debug:
        logger.setLevel(logging.DEBUG)
        for h in logger.handlers:
            h.setLevel(logging.DEBUG)
        logging.getLogger('urllib3').setLevel(logging.WARNING)

    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ë¹ ë¥¸ ê²€ì¦
    if args.test and not os.path.exists('urls_test.txt'):
        logger.error("âŒ í…ŒìŠ¤íŠ¸ ëª¨ë“œì´ì§€ë§Œ urls_test.txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    max_workers = args.workers
    if max_workers < 1:
        max_workers = 1
    elif max_workers > 16:  # ìµœëŒ€ ì œí•œ
        max_workers = 16
        
    # ë‹¤ìš´ë¡œë” ì´ˆê¸°í™”
    downloader = CouncilFileDownloader(use_selenium=args.selenium, max_workers=max_workers)

    # ì¶œë ¥ í´ë”ë¥¼ pdf_data/<íƒ€ì„ìŠ¤íƒ¬í”„> í˜•íƒœë¡œ ê°•ì œ
    ts = downloader.current_date.strftime('%Y%m%d_%H%M')
    out_root = os.path.abspath(args.outdir)
    downloader.base_download_dir = os.path.join(out_root, ts)
    os.makedirs(downloader.base_download_dir, exist_ok=True)

    try:
        downloader.run()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ì‚¬ìš©ì ì¤‘ë‹¨")
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
